nangel3@jp-gpu3 ~/research/convai/ranker $ py test.py models/short_term/1524931098.97_Estimator_                                                                                            [56/1801]
/home/ml/nangel3/.local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it wi
ll be treated as `np.float64 == np.dtype(float).type`.
from ._conv import register_converters as _register_converters

Namespace(gpu=0, short_term_model='models/short_term/1524931098.97_Estimator_')

Loading model models/short_term/1524931098.97_Estimator_ ...
prev. max train accuracies: [0.907245, 0.8077895, 0.8474744, 0.8579095, 0.8525743, 0.8637312, 0.8588824, 0.8417468]
prev. max valid accuracies: [0.5400452, 0.82726586, 0.8642983, 0.86304295, 0.8718303, 0.8637961, 0.86567914, 0.863294]
prev. best avg. train accuracy: 0.854669
prev. best avg. valid accuracy: 0.819906
Building the networks...
WARNING:tensorflow:From /home/ml/nangel3/.local/lib/python2.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:731: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) i$
deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See tf.nn.softmax_cross_entropy_with_logits_v2.

Reset short term network parameters...
Model restored to models/short_term/1524931098.97_Estimator
Testing the short term network...
test accuracy: 0.824592
Get train, valid, test prediction...
[fold 1] train acc: 55111/63727=0.864798
[fold 1] train conf.mtrx:
[[54113   341]
 [ 8275   998]]
[fold 1] valid acc: 4162/7966=0.52247
[fold 1] valid conf.mtrx:
[[4048   40]
 [3764  114]]
[fold 2] train acc: 52391/63727=0.822116
[fold 2] train conf.mtrx:
[[51388   343]
 [10993  1003]]
[fold 2] valid acc: 6882/7966=0.863922
[fold 2] valid conf.mtrx:
[[6773   38]
 [1046  109]]
[fold 3] train acc: 52393/63727=0.822148
[fold 3] train conf.mtrx:
[[51400   335]
 [10999   993]]
[fold 3] valid acc: 6880/7966=0.863671
[fold 3] valid conf.mtrx:
[[6761   46]
 [1040  119]]
[fold 4] train acc: 52390/63727=0.822101                                                                                                                                                     [5/1801]
[fold 4] train conf.mtrx:
[[51411   330]
 [11007   979]]
[fold 4] valid acc: 6883/7966=0.864047
[fold 4] valid conf.mtrx:
[[6750   51]
 [1032  133]]
[fold 5] train acc: 52356/63727=0.821567
[fold 5] train conf.mtrx:
[[51394   343]
 [11028   962]]
[fold 5] valid acc: 6917/7966=0.868315
[fold 5] valid conf.mtrx:
[[6767   38]
 [1011  150]]
[fold 6] train acc: 52384/63727=0.822006
[fold 6] train conf.mtrx:
[[51394   338]
 [11005   990]]
[fold 6] valid acc: 6889/7966=0.8648
[fold 6] valid conf.mtrx:
[[6767   43]
 [1034  122]]
[fold 7] train acc: 52383/63727=0.821991
[fold 7] train conf.mtrx:
[[51396   337]
 [11007   987]]
[fold 7] valid acc: 6890/7966=0.864926
[fold 7] valid conf.mtrx:
[[6765   44]
 [1032  125]]
[fold 8] train acc: 52396/63727=0.822195
[fold 8] train conf.mtrx:
[[51393   339]
 [10992  1003]]
[fold 8] valid acc: 6877/7966=0.863294
[fold 8] valid conf.mtrx:
[[6768   42]
 [1047  109]]

avg. train acc. 0.827365
avg. train conf.mtrx.
[[51736.125   338.25 ]
 [10663.25    989.375]]

avg. valid acc. 0.821931
avg. valid conf.mtrx.
[[6424.875   42.75 ]
 [1375.75   122.625]]

test acc: 6572/7970=0.824592
test acc: 6572/7970=0.824592
[fold 8] valid conf.mtrx:
[[6466   51]
 [1347  106]]
done.

